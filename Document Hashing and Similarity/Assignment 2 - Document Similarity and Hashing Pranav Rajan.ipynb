{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some stuff\n",
    "import numpy as np\n",
    "from math import inf\n",
    "import time as time\n",
    "import hashlib\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and store files\n",
    "def document_reader(file_name):\n",
    "    f = open(file_name, \"r\")\n",
    "    if (f.mode == \"r\"):\n",
    "        contents = f.read()\n",
    "        return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a set of functions that generates k grams based on different criteria and the jaccardian similiary between documents\n",
    "\n",
    "# function that generates 2 character k grams\n",
    "def generate_2_character_gram(document):\n",
    "    # set to ensure that there are no duplicates\n",
    "    two_char_gram_set = set()\n",
    "    \n",
    "    for i in range(0, len(document) - 1):\n",
    "        two_char_string = \"\"\n",
    "        char_1 = document[i]\n",
    "        char_2 = document[i + 1]\n",
    "        two_char_string += char_1 + char_2\n",
    "        two_char_gram_set.add(two_char_string)    \n",
    "    return two_char_gram_set\n",
    "\n",
    "# function that generates 3 character k grams\n",
    "def generate_3_character_gram(document):\n",
    "    # set to ensure that there are no duplicates\n",
    "    three_char_gram_set = set()\n",
    "    \n",
    "    # set to ensure that there are no duplicates\n",
    "    three_char_gram_set = set()\n",
    "    \n",
    "    for j in range(0, len(document) - 2):\n",
    "        three_char_string = \"\"\n",
    "        char_1 = document[j]\n",
    "        char_2 = document[j + 1]\n",
    "        char_3 = document[j + 2]\n",
    "        three_char_string += char_1 + char_2 + char_3\n",
    "        three_char_gram_set.add(three_char_string)\n",
    "        \n",
    "    return three_char_gram_set\n",
    "\n",
    "# function that generates 2 word k grams\n",
    "def generate_2_word_gram(document):    \n",
    "    # split the document into tokens (words)\n",
    "    document_words = document.split()\n",
    "    \n",
    "    # set to ensure that there are no duplicates\n",
    "    two_word_gram_set = set()\n",
    "    \n",
    "    # use the same logic from generate_2_character_gram function to construct the grams\n",
    "    for h in range(0, len(document_words) - 1):\n",
    "        two_word_string = \"\"\n",
    "        word_1 = document_words[h]\n",
    "        word_2 = document_words[h + 1]\n",
    "        two_word_string += word_1 + \" \" + word_2\n",
    "        two_word_gram_set.add(two_word_string)\n",
    "\n",
    "    return two_word_gram_set\n",
    "\n",
    "def jaccardian_similarity(a, b):\n",
    "    # compute the intersection of a and b\n",
    "    a_intersect_b = a.intersection(b)\n",
    "    \n",
    "    # compute the magnitude of the intersection of a intersect b\n",
    "    a_intersect_b_magnitude = len(a_intersect_b)\n",
    "    \n",
    "    # compute the union of a and b\n",
    "    a_union_b = a.union(b)\n",
    "    \n",
    "    # compute the magnitude of the union of a and b\n",
    "    a_union_b_magnitude = len(a_union_b)\n",
    "    \n",
    "    similarity = a_intersect_b_magnitude / a_union_b_magnitude\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a set of functions for experimenting with min hashing \n",
    "\n",
    "# function that generates a bunch of hashed k grams of different lengths\n",
    "def min_hashing(k_grams, m, t):\n",
    "    # generate a number of salt values to generate t number of hash functions\n",
    "    salt_list = []\n",
    "    salt_list.extend(range(t))\n",
    "    \n",
    "    # list that stores k number of lists containing t different hash functions\n",
    "    hash_vector_list = []\n",
    "    \n",
    "    for k in k_grams:\n",
    "        # list for storing t different hash functions\n",
    "        hash_vector = []\n",
    "        \n",
    "        # salt and hash some stuff\n",
    "        for p in range(len(salt_list)):\n",
    "            salt_value = \"\"\n",
    "            salt_value += str(salt_list[p])\n",
    "            \n",
    "            # k_gram + salt for hashing\n",
    "            salted_string = k + salt_value\n",
    "            \n",
    "            # hash salted string\n",
    "            hash_func = hashlib.md5()\n",
    "            hash_func.update(salted_string.encode())\n",
    "            hex_hash = hash_func.hexdigest()\n",
    "            hash_value = int(hex_hash, 16) % m\n",
    "            hash_vector.append(hash_value)\n",
    "            \n",
    "        hash_vector_list.append(hash_vector)\n",
    "        \n",
    "    return hash_vector_list\n",
    "\n",
    "# function that takes a list of lists containing hash values and finds the minimum values\n",
    "def generate_minimum_hash_values(hash_list, t):\n",
    "    min_hash_list = [inf for g in range(t)]\n",
    "    \n",
    "    for h in range(len(hash_list)):\n",
    "        hash_value_list = hash_list[h]\n",
    "        for g in range(t):\n",
    "            if hash_value_list[g] < min_hash_list[g]:\n",
    "                min_hash_list[g] = hash_value_list[g]\n",
    "    return min_hash_list\n",
    "\n",
    "# function that generates the jaccardian similarity for min hashing\n",
    "def min_hashing_jaccardian_similarity(a, b, t):\n",
    "    # keeping track of the sum\n",
    "    sum = 0\n",
    "    comparison_list = []\n",
    "    for d in range(len(a)):\n",
    "        if a[d] == b[d]:\n",
    "            comparison_list.append(1)\n",
    "        else:\n",
    "            comparison_list.append(0)\n",
    "    for p in range(len(comparison_list)):\n",
    "        sum += comparison_list[p]\n",
    "    return sum / t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of distinct 2 character grams for document 1: 266\n",
      "The number of distinct 2 character grams for document 2: 264\n",
      "The number of distinct 2 character grams for document 3: 296\n",
      "The number of distinct 2 character grams for document 4: 249\n",
      "\n",
      "The number of distinct 3 character grams for document 1: 770\n",
      "The number of distinct 3 character grams for document 2: 759\n",
      "The number of distinct 3 character grams for document 3: 978\n",
      "The number of distinct 3 character grams for document 4: 770\n",
      "\n",
      "The number of distinct 2 word grams for document 1: 289\n",
      "The number of distinct 2 word grams for document 2: 297\n",
      "The number of distinct 2 word grams for document 3: 390\n",
      "The number of distinct 2 word grams for document 4: 364\n",
      "\n",
      "The jaccardian similarity for 2 character grams for document 1 and document 2: 0.9924812030075187\n",
      "The jaccardian similarity for 2 character grams for document 1 and docuemnt 3: 0.7841269841269841\n",
      "The jaccardian similarity for 2 character grams for document 1 and document 4: 0.6666666666666666\n",
      "The jaccardian similarity for 2 character grams for document 2 and document 3: 0.7834394904458599\n",
      "The jaccardian similarity for 2 character grams for document 2 and document 4: 0.6601941747572816\n",
      "The jaccardian similarity for 2 character grams for document 3 and document 4: 0.6717791411042945\n",
      "\n",
      "The jaccardian similarity for 3 character grams for document 1 and document 2: 0.9552429667519181\n",
      "The jaccardian similarity for 3 character grams for document 1 and document 3: 0.5030094582975064\n",
      "The jaccardian similarity for 3 character grams for document 1 and document 4: 0.3061916878710772\n",
      "The jaccardian similarity for 3 character grams for document 2 and document 3: 0.4987057808455565\n",
      "The jaccardian similarity for 3 character grams for document 2 and document 4: 0.3034953111679454\n",
      "The jaccardian similarity for 3 character grams for document 3 and document 4: 0.31329827197595794\n",
      "\n",
      "The jaccardian similarity for 2 word grams for document 1 and document 2: 0.7920489296636085\n",
      "The jaccardian similarity for 2 word grams for document 1 and document 3: 0.1954225352112676\n",
      "The jaccardian similarity for 2 word grams for document 1 and document 4: 0.007716049382716049\n",
      "The jaccardian similarity for 2 word grams for document 2 and document 3: 0.17636986301369864\n",
      "The jaccardian similarity for 2 word grams for document 2 and document 4: 0.00916030534351145\n",
      "The jaccardian similarity for 2 word grams for document 3 and document 4: 0.012080536912751677\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "\n",
    "# Read documents\n",
    "doc_1 = document_reader(\"D1.txt\")\n",
    "doc_2 = document_reader(\"D2.txt\")\n",
    "doc_3 = document_reader(\"D3.txt\")\n",
    "doc_4 = document_reader(\"D4.txt\")\n",
    "\n",
    "# print statements for debugging the documents\n",
    "# print(doc_1)\n",
    "# print(doc_2)\n",
    "# print(doc_3)\n",
    "# print(doc_4)\n",
    "\n",
    "# Generate the different 2 character grams for the documents\n",
    "doc_1_2_char_gram = generate_2_character_gram(doc_1)\n",
    "doc_2_2_char_gram = generate_2_character_gram(doc_2)\n",
    "doc_3_2_char_gram = generate_2_character_gram(doc_3)\n",
    "doc_4_2_char_gram = generate_2_character_gram(doc_4)\n",
    "\n",
    "# Generate the different 3 character grams for the documents\n",
    "doc_1_3_char_gram = generate_3_character_gram(doc_1)\n",
    "doc_2_3_char_gram = generate_3_character_gram(doc_2)\n",
    "doc_3_3_char_gram = generate_3_character_gram(doc_3)\n",
    "doc_4_3_char_gram = generate_3_character_gram(doc_4)\n",
    "\n",
    "# Generate the different 2 word grams for the documents\n",
    "doc_1_2_word_gram = generate_2_word_gram(doc_1)\n",
    "doc_2_2_word_gram = generate_2_word_gram(doc_2)\n",
    "doc_3_2_word_gram = generate_2_word_gram(doc_3)\n",
    "doc_4_2_word_gram = generate_2_word_gram(doc_4)\n",
    "\n",
    "# Compute the jaccardian similarity between the documents\n",
    "\n",
    "# 2 character gram similarities\n",
    "doc1_doc2_2_char = jaccardian_similarity(doc_1_2_char_gram, doc_2_2_char_gram)\n",
    "doc1_doc3_2_char = jaccardian_similarity(doc_1_2_char_gram, doc_3_2_char_gram)\n",
    "doc1_doc4_2_char = jaccardian_similarity(doc_1_2_char_gram, doc_4_2_char_gram)\n",
    "doc2_doc3_2_char = jaccardian_similarity(doc_2_2_char_gram, doc_3_2_char_gram)\n",
    "doc2_doc4_2_char = jaccardian_similarity(doc_2_2_char_gram, doc_4_2_char_gram)\n",
    "doc3_doc4_2_char = jaccardian_similarity(doc_3_2_char_gram, doc_4_2_char_gram)\n",
    "\n",
    "# 3 character gram similarities\n",
    "doc1_doc2_3_char = jaccardian_similarity(doc_1_3_char_gram, doc_2_3_char_gram)\n",
    "doc1_doc3_3_char = jaccardian_similarity(doc_1_3_char_gram, doc_3_3_char_gram)\n",
    "doc1_doc4_3_char = jaccardian_similarity(doc_1_3_char_gram, doc_4_3_char_gram)\n",
    "doc2_doc3_3_char = jaccardian_similarity(doc_2_3_char_gram, doc_3_3_char_gram)\n",
    "doc2_doc4_3_char = jaccardian_similarity(doc_2_3_char_gram, doc_4_3_char_gram)\n",
    "doc3_doc4_3_char = jaccardian_similarity(doc_3_3_char_gram, doc_4_3_char_gram)\n",
    "\n",
    "# 2 word gram similarities\n",
    "doc1_doc2_2_word = jaccardian_similarity(doc_1_2_word_gram, doc_2_2_word_gram)\n",
    "doc1_doc3_2_word = jaccardian_similarity(doc_1_2_word_gram, doc_3_2_word_gram)\n",
    "doc1_doc4_2_word = jaccardian_similarity(doc_1_2_word_gram, doc_4_2_word_gram)\n",
    "doc2_doc3_2_word = jaccardian_similarity(doc_2_2_word_gram, doc_3_2_word_gram)\n",
    "doc2_doc4_2_word = jaccardian_similarity(doc_2_2_word_gram, doc_4_2_word_gram)\n",
    "doc3_doc4_2_word = jaccardian_similarity(doc_3_2_word_gram, doc_4_2_word_gram)\n",
    "\n",
    "# Generate the results\n",
    "print(f\"The number of distinct 2 character grams for document 1: {len(doc_1_2_char_gram)}\")\n",
    "print(f\"The number of distinct 2 character grams for document 2: {len(doc_2_2_char_gram)}\")\n",
    "print(f\"The number of distinct 2 character grams for document 3: {len(doc_3_2_char_gram)}\")\n",
    "print(f\"The number of distinct 2 character grams for document 4: {len(doc_4_2_char_gram)}\")\n",
    "print()\n",
    "print(f\"The number of distinct 3 character grams for document 1: {len(doc_1_3_char_gram)}\")\n",
    "print(f\"The number of distinct 3 character grams for document 2: {len(doc_2_3_char_gram)}\")\n",
    "print(f\"The number of distinct 3 character grams for document 3: {len(doc_3_3_char_gram)}\")\n",
    "print(f\"The number of distinct 3 character grams for document 4: {len(doc_4_3_char_gram)}\")\n",
    "print()\n",
    "print(f\"The number of distinct 2 word grams for document 1: {len(doc_1_2_word_gram)}\")\n",
    "print(f\"The number of distinct 2 word grams for document 2: {len(doc_2_2_word_gram)}\")\n",
    "print(f\"The number of distinct 2 word grams for document 3: {len(doc_3_2_word_gram)}\")\n",
    "print(f\"The number of distinct 2 word grams for document 4: {len(doc_4_2_word_gram)}\")\n",
    "print()\n",
    "print(f\"The jaccardian similarity for 2 character grams for document 1 and document 2: {doc1_doc2_2_char}\")\n",
    "print(f\"The jaccardian similarity for 2 character grams for document 1 and docuemnt 3: {doc1_doc3_2_char}\")\n",
    "print(f\"The jaccardian similarity for 2 character grams for document 1 and document 4: {doc1_doc4_2_char}\")\n",
    "print(f\"The jaccardian similarity for 2 character grams for document 2 and document 3: {doc2_doc3_2_char}\")\n",
    "print(f\"The jaccardian similarity for 2 character grams for document 2 and document 4: {doc2_doc4_2_char}\")\n",
    "print(f\"The jaccardian similarity for 2 character grams for document 3 and document 4: {doc3_doc4_2_char}\")\n",
    "print()\n",
    "print(f\"The jaccardian similarity for 3 character grams for document 1 and document 2: {doc1_doc2_3_char}\")\n",
    "print(f\"The jaccardian similarity for 3 character grams for document 1 and document 3: {doc1_doc3_3_char}\")\n",
    "print(f\"The jaccardian similarity for 3 character grams for document 1 and document 4: {doc1_doc4_3_char}\")\n",
    "print(f\"The jaccardian similarity for 3 character grams for document 2 and document 3: {doc2_doc3_3_char}\")\n",
    "print(f\"The jaccardian similarity for 3 character grams for document 2 and document 4: {doc2_doc4_3_char}\")\n",
    "print(f\"The jaccardian similarity for 3 character grams for document 3 and document 4: {doc3_doc4_3_char}\")\n",
    "print()\n",
    "print(f\"The jaccardian similarity for 2 word grams for document 1 and document 2: {doc1_doc2_2_word}\")\n",
    "print(f\"The jaccardian similarity for 2 word grams for document 1 and document 3: {doc1_doc3_2_word}\")\n",
    "print(f\"The jaccardian similarity for 2 word grams for document 1 and document 4: {doc1_doc4_2_word}\")\n",
    "print(f\"The jaccardian similarity for 2 word grams for document 2 and document 3: {doc2_doc3_2_word}\")\n",
    "print(f\"The jaccardian similarity for 2 word grams for document 2 and document 4: {doc2_doc4_2_word}\")\n",
    "print(f\"The jaccardian similarity for 2 word grams for document 3 and document 4: {doc3_doc4_2_word}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current t value is: 20\n",
      "The jaccard similarity for min hashed document 1 and min hashed document 2 for 20 hash functions is: 0.9\n",
      "the current t value is: 60\n",
      "The jaccard similarity for min hashed document 1 and min hashed document 2 for 60 hash functions is: 0.9166666666666666\n",
      "the current t value is: 150\n",
      "The jaccard similarity for min hashed document 1 and min hashed document 2 for 150 hash functions is: 0.9333333333333333\n",
      "the current t value is: 300\n",
      "The jaccard similarity for min hashed document 1 and min hashed document 2 for 300 hash functions is: 0.9333333333333333\n",
      "the current t value is: 600\n",
      "The jaccard similarity for min hashed document 1 and min hashed document 2 for 600 hash functions is: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Question 2\n",
    "# Read documents\n",
    "doc_1 = document_reader(\"D1.txt\")\n",
    "doc_2 = document_reader(\"D2.txt\")\n",
    "\n",
    "# generate the sets of character grams for document 1 and document 2\n",
    "doc_1_3_char_gram = generate_3_character_gram(doc_1)\n",
    "doc_2_3_char_gram = generate_3_character_gram(doc_2)\n",
    "\n",
    "# the number of bins for the hash table\n",
    "m = 10000\n",
    "\n",
    "# list that contains t values\n",
    "t_list = [20, 60, 150, 300, 600]\n",
    "\n",
    "for w in range(len(t_list)):\n",
    "    t_value = t_list[w]\n",
    "    print(f\"the current t value is: {t_value}\")\n",
    "    \n",
    "    # compute the hashed strings\n",
    "    document_1_hash_vector = min_hashing(doc_1_3_char_gram, m, t_value)\n",
    "    document_2_hash_vector = min_hashing(doc_2_3_char_gram, m, t_value)\n",
    "    \n",
    "    # compute the minimum hash values\n",
    "    document_1_min_hash_vector = generate_minimum_hash_values(document_1_hash_vector, t_value)\n",
    "    document_2_min_hash_vector = generate_minimum_hash_values(document_2_hash_vector, t_value)\n",
    "    \n",
    "    # compute the jaccardian similarity for the minhashed documents\n",
    "    jaccard_min_hash_similarity = min_hashing_jaccardian_similarity(document_1_min_hash_vector, document_2_min_hash_vector, t_value)\n",
    "    \n",
    "    # generate the result\n",
    "    print(f\"The jaccard similarity for min hashed document 1 and min hashed document 2 for {t_value} hash functions is: {jaccard_min_hash_similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current t value is: 20\n",
      "The jaccardian minhash similarity result: 0.9\n",
      "The time taken for 20 hash functions is: 0.21764087677001953\n",
      "the current t value is: 60\n",
      "The jaccardian minhash similarity result: 0.9166666666666666\n",
      "The time taken for 60 hash functions is: 0.6124279499053955\n",
      "the current t value is: 150\n",
      "The jaccardian minhash similarity result: 0.9333333333333333\n",
      "The time taken for 150 hash functions is: 1.3975067138671875\n",
      "the current t value is: 300\n",
      "The jaccardian minhash similarity result: 0.9333333333333333\n",
      "The time taken for 300 hash functions is: 2.8767662048339844\n",
      "the current t value is: 600\n",
      "The jaccardian minhash similarity result: 0.95\n",
      "The time taken for 600 hash functions is: 5.464832544326782\n",
      "the current t value is: 800\n",
      "The jaccardian minhash similarity result: 0.93875\n",
      "The time taken for 800 hash functions is: 7.227124929428101\n",
      "the current t value is: 1000\n",
      "The jaccardian minhash similarity result: 0.943\n",
      "The time taken for 1000 hash functions is: 8.845203399658203\n",
      "the current t value is: 1300\n",
      "The jaccardian minhash similarity result: 0.9446153846153846\n",
      "The time taken for 1300 hash functions is: 11.36416745185852\n",
      "the current t value is: 1500\n",
      "The jaccardian minhash similarity result: 0.9446666666666667\n",
      "The time taken for 1500 hash functions is: 13.356303930282593\n",
      "the current t value is: 2000\n",
      "The jaccardian minhash similarity result: 0.9465\n",
      "The time taken for 2000 hash functions is: 18.416617393493652\n",
      "the current t value is: 2500\n",
      "The jaccardian minhash similarity result: 0.9468\n",
      "The time taken for 2500 hash functions is: 22.74419093132019\n",
      "the current t value is: 3000\n",
      "The jaccardian minhash similarity result: 0.9433333333333334\n",
      "The time taken for 3000 hash functions is: 27.251686573028564\n",
      "the current t value is: 3500\n",
      "The jaccardian minhash similarity result: 0.9465714285714286\n",
      "The time taken for 3500 hash functions is: 32.488582372665405\n",
      "the current t value is: 4000\n",
      "The jaccardian minhash similarity result: 0.94775\n",
      "The time taken for 4000 hash functions is: 36.77555561065674\n",
      "the current t value is: 4500\n",
      "The jaccardian minhash similarity result: 0.9462222222222222\n",
      "The time taken for 4500 hash functions is: 40.810161113739014\n",
      "the current t value is: 5000\n",
      "The jaccardian minhash similarity result: 0.947\n",
      "The time taken for 5000 hash functions is: 44.326783895492554\n",
      "the current t value is: 5500\n",
      "The jaccardian minhash similarity result: 0.9481818181818182\n",
      "The time taken for 5500 hash functions is: 49.294180154800415\n",
      "the current t value is: 10000\n",
      "The jaccardian minhash similarity result: 0.9536\n",
      "The time taken for 10000 hash functions is: 90.2963559627533\n",
      "the current t value is: 15000\n",
      "The jaccardian minhash similarity result: 0.9544\n",
      "The time taken for 15000 hash functions is: 135.15026569366455\n",
      "the current t value is: 20000\n",
      "The jaccardian minhash similarity result: 0.954\n",
      "The time taken for 20000 hash functions is: 179.21107649803162\n"
     ]
    }
   ],
   "source": [
    "# Timing and Accuracy for min hashing experiments\n",
    "\n",
    "# Question 2\n",
    "# Read documents\n",
    "doc_1 = document_reader(\"D1.txt\")\n",
    "doc_2 = document_reader(\"D2.txt\")\n",
    "\n",
    "# generate the sets of character grams for document 1 and document 2\n",
    "doc_1_3_char_gram = generate_3_character_gram(doc_1)\n",
    "doc_2_3_char_gram = generate_3_character_gram(doc_2)\n",
    "\n",
    "# the number of bins for the hash table\n",
    "m = 10000\n",
    "\n",
    "# list that contains t values\n",
    "t_list = [20, 60, 150, 300, 600, 800, 1000, 1300, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 10000, 15000, 20000]\n",
    "\n",
    "def time_min_hash(t_value):\n",
    "    # start the clock\n",
    "    start_time = time.time()\n",
    "    # compute the hashed strings\n",
    "    document_1_hash_vector = min_hashing(doc_1_3_char_gram, m, t_value)\n",
    "    document_2_hash_vector = min_hashing(doc_2_3_char_gram, m, t_value)\n",
    "    \n",
    "    # compute the minimum hash values\n",
    "    document_1_min_hash_vector = generate_minimum_hash_values(document_1_hash_vector, t_value)\n",
    "    document_2_min_hash_vector = generate_minimum_hash_values(document_2_hash_vector, t_value)\n",
    "    \n",
    "    # compute the jaccardian similarity for the minhashed documents\n",
    "    jaccard_min_hash_similarity = min_hashing_jaccardian_similarity(document_1_min_hash_vector, document_2_min_hash_vector, t_value)\n",
    "    \n",
    "    # compute the time delta\n",
    "    delta_time = time.time() - start_time\n",
    "    print(f\"The jaccardian minhash similarity result: {jaccard_min_hash_similarity}\")\n",
    "    \n",
    "    return delta_time\n",
    "    \n",
    "    \n",
    "for w in range(len(t_list)):\n",
    "    t_value = t_list[w]\n",
    "    print(f\"the current t value is: {t_value}\")\n",
    "    time_val = time_min_hash(t_value)\n",
    "    print(f\"The time taken for {t_value} hash functions is: {time_val}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
